{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JFiFWQCUfpbd"
   },
   "source": [
    "<img src=\"./images/DLI_Header.png\" style=\"width: 400px;\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MePd6N62fpbo"
   },
   "source": [
    "# 6.0 Mixture of Experts (MoE)\n",
    "\n",
    "이번 노트북에서는 Mixture of Experts 모델 트레이닝에 대해 알아봅니다.\n",
    "\n",
    "## 목표 \n",
    "\n",
    "이번 노트북의 목표는 다음과 같습니다 :\n",
    "* 심플 컨볼루션 네트워크에서 선형 전문가(expert) 신경망들을 통합하는 방법\n",
    "* 분류 문제를 위해 새로운 Mixture of Experts CNN을 트레이닝하는 방법\n",
    "\n",
    "\n",
    "### 기존 실행 중이거나 보류 상태의 Job을 취소하고 이번 노트북을 진행합니다. \n",
    "\n",
    "실습 진행에 앞서  SLURM 대기열에서 아직 실행 중이거나 대기 중인 작업이 없는지 확인하십시오. 다음 셀을 실행하여 SLURM 작업 대기열을 확인합니다. :\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "l1pFYLm2fpbr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHZi_RmUfpbu"
   },
   "source": [
    "아직 실행 중이거나 보류 중인 작업이 있는 경우 다음 셀을 실행하고 `scancel` 명령을 사용하여 모든 사용자의 작업을 취소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 43,
     "status": "ok",
     "timestamp": 1661300640950,
     "user": {
      "displayName": "Solee Moon KR",
      "userId": "15448835692925759007"
     },
     "user_tz": -540
    },
    "id": "H7mwmTCsfpbv",
    "outputId": "77c4a51d-2895-406c-d27a-3328b093fb51"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QTutrNY3fpby"
   },
   "source": [
    "---\n",
    "# 6.1 Mixture of Experts 개요\n",
    "\n",
    "Mixture of Experts(MoE)은 일부 레이어가 상황에 따라 활성화되거나 활성화되지 않는 작은 그룹으로 분할되는 신경망입니다.\n",
    "이 구조는 네트워크가 더 넓은 범위의 동작을 학습할 수 있게 합니다. 또 다른 장점은 한 번에 소수의 Expert 신경망들만 활동하기 때문에 MoE 모델이 더 적은 연산을 필요로 한다는 것입니다.\n",
    "\n",
    "<img src=\"images/MOE.png\" width=\"450\" />\n",
    "\n",
    "최근 논문에서는 [Switch Transformer](https://arxiv.org/pdf/2101.03961.pdf)와 같은 MoE 구조를 따르는 여러 모델이 개발되었습니다.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GWv2UzCgfpb0"
   },
   "source": [
    "# 6.2 베이스라인 CNN을 위한 MoE 작성 방법\n",
    "\n",
    "CNN cifar-10 분류기 모델로 돌아가서 MoE 레이어 1개를 추가하도록 수정해 보겠습니다. CNN 컨볼루션 레이어들은 특징들을 추출하며, 나중에 완전히 연결된 레이어는 CIFAR-10 분류 문제에 특화되어 있습니다. \n",
    "네트워크 정의에 전문가 계층을 추가하려면 다음과 같이`deepspeed.moe.layer.MoE`를 사용하십시오 (정방향 패스를 수정합니다).:\n",
    "\n",
    "```\n",
    "deepspeed.moe.layer.MoE( hidden_size=<Hidden dimension of the model>, \n",
    "                         expert=<Torch module that defines the expert>, \n",
    "                         num_experts=<Desired number of expert>, \n",
    "                         ep_size=<Desired expert-parallel world size>,\n",
    "                         ...\n",
    "                         )\n",
    "                         \n",
    "```\n",
    "\n",
    "[DeepSpeed 설명서](https://deepspeed.readthedocs.io/en/latest/moe.html)에서 DeepSpeed의 Mixture of Experts에 대해서 더 알아보세요.\n",
    "\n",
    "초기 레이어에서 추출된 특징을 평가하기 위해 최신의 완전 연결 레이어 `fc3` 를 MoE 레이어로 변환해 보겠습니다. 최종 분류자인 `fc4`를 추가하겠습니다.\n",
    "\n",
    "우리는 이미 [cifar10_deepspeed_MOE.py](./code/moe/cifar10_deepspeed_MOE.py)스크립트를 준비했습니다. 4개의 GPU에 분할된 8개의 전문가 신경망을 사용하여 실행해 보겠습니다. 다시 말하면, 각 GPU는 2개의 전문가 신경망을 처리할 예정입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 31,
     "status": "ok",
     "timestamp": 1661300640952,
     "user": {
      "displayName": "Solee Moon KR",
      "userId": "15448835692925759007"
     },
     "user_tz": -540
    },
    "id": "cPeSjt6ffpb5",
    "outputId": "3fff3970-fb3e-4833-8942-54c743a53145",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-03-28 05:17:18,599] [WARNING] [runner.py:159:fetch_hostfile] Unable to find hostfile, will proceed with training with local resources only.\n",
      "[2025-03-28 05:17:18,765] [INFO] [runner.py:457:main] cmd = /opt/conda/bin/python3.8 -u -m deepspeed.launcher.launch --world_info=eyJsb2NhbGhvc3QiOiBbMCwgMSwgMiwgM119 --master_addr=127.0.0.1 --master_port=29500 /dli/code/moe/cifar10_deepspeed_MOE.py --deepspeed --deepspeed_config /dli/code/moe/ds_config.json --moe --ep-world-size 4 --num-experts-per-layer 8 --top-k 1 --noisy-gate-policy RSample --moe-param-group --profile-execution=True --profile-name=zero0_MOE\n",
      "[2025-03-28 05:17:20,309] [INFO] [launch.py:96:main] 0 NCCL_P2P_DISABLE=1\n",
      "[2025-03-28 05:17:20,309] [INFO] [launch.py:96:main] 0 NCCL_VERSION=2.11.4\n",
      "[2025-03-28 05:17:20,309] [INFO] [launch.py:103:main] WORLD INFO DICT: {'localhost': [0, 1, 2, 3]}\n",
      "[2025-03-28 05:17:20,310] [INFO] [launch.py:109:main] nnodes=1, num_local_procs=4, node_rank=0\n",
      "[2025-03-28 05:17:20,310] [INFO] [launch.py:122:main] global_rank_mapping=defaultdict(<class 'list'>, {'localhost': [0, 1, 2, 3]})\n",
      "[2025-03-28 05:17:20,310] [INFO] [launch.py:123:main] dist_world_size=4\n",
      "[2025-03-28 05:17:20,310] [INFO] [launch.py:125:main] Setting CUDA_VISIBLE_DEVICES=0,1,2,3\n",
      "[2025-03-28 05:17:22,234] [INFO] [distributed.py:48:init_distributed] Initializing torch distributed with backend: nccl\n",
      "Files already downloaded and verified\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "\n",
      "[2025-03-28 05:17:28,160] [INFO] [logging.py:69:log_dist] [Rank 0] Creating MoE layer with num_experts: 8 | num_local_experts: 2 | expert_parallel_size: 4\n",
      "[2025-03-28 05:17:28,162] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed info: version=0.6.5, git-hash=unknown, git-branch=unknown\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2025-03-28 05:17:28,167] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert and data parallel groups with size 4\n",
      "Files already downloaded and verifiedFiles already downloaded and verifiedFiles already downloaded and verified\n",
      "\n",
      "\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "No existing process group found, creating a new group named: ep_size_4\n",
      "[2025-03-28 05:17:29,221] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [0]\n",
      "[2025-03-28 05:17:29,221] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [1]\n",
      "[2025-03-28 05:17:29,232] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [2]\n",
      "[2025-03-28 05:17:29,243] [INFO] [logging.py:69:log_dist] [Rank 0] Creating expert data parallel process group named ep_size_4 with ranks: [3]\n",
      "[2025-03-28 05:17:29,254] [INFO] [logging.py:69:log_dist] [Rank 0] creating expert parallel process group named ep_size_4 with ranks: [0, 1, 2, 3]\n",
      "[2025-03-28 05:17:29,464] [INFO] [engine.py:278:__init__] DeepSpeed Flops Profiler Enabled: False\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/fused_adam/build.ninja...\n",
      "Building extension module fused_adam...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.17894220352172852 seconds\n",
      "[2025-03-28 05:17:30,319] [INFO] [engine.py:1100:_configure_optimizer] Using DeepSpeed Optimizer param name adam as basic optimizer\n",
      "[2025-03-28 05:17:30,320] [INFO] [engine.py:1108:_configure_optimizer] DeepSpeed Basic Optimizer = FusedAdam\n",
      "[2025-03-28 05:17:30,320] [INFO] [logging.py:69:log_dist] [Rank 0] Creating fp16 optimizer with dynamic loss scale\n",
      "[2025-03-28 05:17:30,325] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed Final Optimizer = adam\n",
      "[2025-03-28 05:17:30,325] [INFO] [engine.py:785:_configure_lr_scheduler] DeepSpeed using configured LR scheduler = WarmupLR\n",
      "[2025-03-28 05:17:30,325] [INFO] [logging.py:69:log_dist] [Rank 0] DeepSpeed LR Scheduler = <deepspeed.runtime.lr_schedules.WarmupLR object at 0x7f2ec7c56bb0>\n",
      "[2025-03-28 05:17:30,325] [INFO] [logging.py:69:log_dist] [Rank 0] step=0, skipped=0, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2025-03-28 05:17:30,325] [INFO] [config.py:1059:print] DeepSpeedEngine configuration:\n",
      "[2025-03-28 05:17:30,326] [INFO] [config.py:1063:print]   activation_checkpointing_config  {\n",
      "    \"partition_activations\": false, \n",
      "    \"contiguous_memory_optimization\": false, \n",
      "    \"cpu_checkpointing\": false, \n",
      "    \"number_checkpoints\": null, \n",
      "    \"synchronize_checkpoint_boundary\": false, \n",
      "    \"profile\": false\n",
      "}\n",
      "[2025-03-28 05:17:30,326] [INFO] [config.py:1063:print]   aio_config ................... {'block_size': 1048576, 'queue_depth': 8, 'thread_count': 1, 'single_submit': False, 'overlap_events': True}\n",
      "[2025-03-28 05:17:30,326] [INFO] [config.py:1063:print]   amp_enabled .................. False\n",
      "[2025-03-28 05:17:30,326] [INFO] [config.py:1063:print]   amp_params ................... False\n",
      "[2025-03-28 05:17:30,326] [INFO] [config.py:1063:print]   autotuning_config ............ {\n",
      "    \"enabled\": false, \n",
      "    \"start_step\": null, \n",
      "    \"end_step\": null, \n",
      "    \"metric_path\": null, \n",
      "    \"arg_mappings\": null, \n",
      "    \"metric\": \"throughput\", \n",
      "    \"model_info\": null, \n",
      "    \"results_dir\": null, \n",
      "    \"exps_dir\": null, \n",
      "    \"overwrite\": true, \n",
      "    \"fast\": true, \n",
      "    \"start_profile_step\": 3, \n",
      "    \"end_profile_step\": 5, \n",
      "    \"tuner_type\": \"gridsearch\", \n",
      "    \"tuner_early_stopping\": 5, \n",
      "    \"tuner_num_trials\": 50, \n",
      "    \"model_info_path\": null, \n",
      "    \"mp_size\": 1, \n",
      "    \"max_train_batch_size\": null, \n",
      "    \"min_train_batch_size\": 1, \n",
      "    \"max_train_micro_batch_size_per_gpu\": 1.024000e+03, \n",
      "    \"min_train_micro_batch_size_per_gpu\": 1, \n",
      "    \"num_tuning_micro_batch_sizes\": 3\n",
      "}\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   bfloat16_enabled ............. False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   checkpoint_tag_validation_enabled  True\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   checkpoint_tag_validation_fail  False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   communication_data_type ...... None\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   curriculum_enabled ........... False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   curriculum_params ............ False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   dataloader_drop_last ......... False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   disable_allgather ............ False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   dump_state ................... False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   dynamic_loss_scale_args ...... {'init_scale': 32768, 'scale_window': 500, 'delayed_shift': 2, 'min_scale': 1}\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_enabled ........... False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_gas_boundary_resolution  1\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_layer_name ........ bert.encoder.layer\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_layer_num ......... 0\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_max_iter .......... 100\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_stability ......... 1e-06\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_tol ............... 0.01\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   eigenvalue_verbose ........... False\n",
      "[2025-03-28 05:17:30,327] [INFO] [config.py:1063:print]   elasticity_enabled ........... False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   flops_profiler_config ........ {\n",
      "    \"enabled\": false, \n",
      "    \"profile_step\": 1, \n",
      "    \"module_depth\": -1, \n",
      "    \"top_modules\": 1, \n",
      "    \"detailed\": true, \n",
      "    \"output_file\": null\n",
      "}\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   fp16_enabled ................. True\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   fp16_master_weights_and_gradients  False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   fp16_mixed_quantize .......... False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   global_rank .................. 0\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   gradient_accumulation_steps .. 1\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   gradient_clipping ............ 1.0\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   gradient_predivide_factor .... 1.0\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   initial_dynamic_scale ........ 32768\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   loss_scale ................... 0\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   memory_breakdown ............. False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   optimizer_legacy_fusion ...... False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   optimizer_name ............... adam\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   optimizer_params ............. {'lr': 0.001, 'betas': [0.8, 0.999], 'eps': 1e-08, 'weight_decay': 3e-07}\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   pipeline ..................... {'stages': 'auto', 'partition': 'best', 'seed_layers': False, 'activation_checkpoint_interval': 0}\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   pld_enabled .................. False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   pld_params ................... False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   prescale_gradients ........... False\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   quantize_change_rate ......... 0.001\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   quantize_groups .............. 1\n",
      "[2025-03-28 05:17:30,328] [INFO] [config.py:1063:print]   quantize_offset .............. 1000\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_period .............. 1000\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_rounding ............ 0\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_start_bits .......... 16\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_target_bits ......... 8\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_training_enabled .... False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_type ................ 0\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   quantize_verbose ............. False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   scheduler_name ............... WarmupLR\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   scheduler_params ............. {'warmup_min_lr': 0, 'warmup_max_lr': 0.001, 'warmup_num_steps': 1000}\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   sparse_attention ............. None\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   sparse_gradients_enabled ..... False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   steps_per_print .............. 2000\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   tensorboard_enabled .......... False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   tensorboard_job_name ......... DeepSpeedJobName\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   tensorboard_output_path ...... \n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   train_batch_size ............. 16\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   train_micro_batch_size_per_gpu  4\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   use_quantizer_kernel ......... False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   wall_clock_breakdown ......... False\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   world_size ................... 4\n",
      "[2025-03-28 05:17:30,329] [INFO] [config.py:1063:print]   zero_allow_untested_optimizer  False\n",
      "[2025-03-28 05:17:30,330] [INFO] [config.py:1063:print]   zero_config .................. {\n",
      "    \"stage\": 0, \n",
      "    \"contiguous_gradients\": true, \n",
      "    \"reduce_scatter\": true, \n",
      "    \"reduce_bucket_size\": 5.000000e+07, \n",
      "    \"allgather_partitions\": true, \n",
      "    \"allgather_bucket_size\": 5.000000e+07, \n",
      "    \"overlap_comm\": true, \n",
      "    \"load_from_fp32_weights\": true, \n",
      "    \"elastic_checkpoint\": false, \n",
      "    \"offload_param\": null, \n",
      "    \"offload_optimizer\": null, \n",
      "    \"sub_group_size\": 1.000000e+09, \n",
      "    \"prefetch_bucket_size\": 5.000000e+07, \n",
      "    \"param_persistence_threshold\": 1.000000e+05, \n",
      "    \"max_live_parameters\": 1.000000e+09, \n",
      "    \"max_reuse_distance\": 1.000000e+09, \n",
      "    \"gather_16bit_weights_on_model_save\": false, \n",
      "    \"ignore_unused_parameters\": true, \n",
      "    \"round_robin_gradients\": false, \n",
      "    \"legacy_stage1\": false\n",
      "}\n",
      "[2025-03-28 05:17:30,330] [INFO] [config.py:1063:print]   zero_enabled ................. False\n",
      "[2025-03-28 05:17:30,330] [INFO] [config.py:1063:print]   zero_optimization_stage ...... 0\n",
      "[2025-03-28 05:17:30,330] [INFO] [config.py:1065:print]   json = {\n",
      "    \"train_batch_size\": 16, \n",
      "    \"steps_per_print\": 2.000000e+03, \n",
      "    \"optimizer\": {\n",
      "        \"type\": \"Adam\", \n",
      "        \"params\": {\n",
      "            \"lr\": 0.001, \n",
      "            \"betas\": [0.8, 0.999], \n",
      "            \"eps\": 1e-08, \n",
      "            \"weight_decay\": 3e-07\n",
      "        }\n",
      "    }, \n",
      "    \"scheduler\": {\n",
      "        \"type\": \"WarmupLR\", \n",
      "        \"params\": {\n",
      "            \"warmup_min_lr\": 0, \n",
      "            \"warmup_max_lr\": 0.001, \n",
      "            \"warmup_num_steps\": 1000\n",
      "        }\n",
      "    }, \n",
      "    \"gradient_clipping\": 1.0, \n",
      "    \"prescale_gradients\": false, \n",
      "    \"fp16\": {\n",
      "        \"enabled\": true, \n",
      "        \"fp16_master_weights_and_grads\": false, \n",
      "        \"loss_scale\": 0, \n",
      "        \"loss_scale_window\": 500, \n",
      "        \"hysteresis\": 2, \n",
      "        \"min_loss_scale\": 1, \n",
      "        \"initial_scale_power\": 15\n",
      "    }, \n",
      "    \"wall_clock_breakdown\": false, \n",
      "    \"zero_optimization\": {\n",
      "        \"stage\": 0, \n",
      "        \"allgather_partitions\": true, \n",
      "        \"reduce_scatter\": true, \n",
      "        \"allgather_bucket_size\": 5.000000e+07, \n",
      "        \"reduce_bucket_size\": 5.000000e+07, \n",
      "        \"overlap_comm\": true, \n",
      "        \"contiguous_gradients\": true, \n",
      "        \"cpu_offload\": false\n",
      "    }\n",
      "}\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20232176780700684 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Loading extension module fused_adam...\n",
      "Loading extension module fused_adam...\n",
      "Time to load fused_adam op: 0.20271611213684082 seconds\n",
      "Time to load fused_adam op: 0.20216965675354004 seconds\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Using /home/admin/.cache/torch_extensions/py38_cu115 as PyTorch extensions root...\n",
      "Emitting ninja build file /home/admin/.cache/torch_extensions/py38_cu115/utils/build.ninja...\n",
      "Building extension module utils...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "ninja: no work to do.\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.18085908889770508 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.1019444465637207 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.10205936431884766 seconds\n",
      "Loading extension module utils...\n",
      "Time to load utils op: 0.20275020599365234 seconds\n",
      "[W CPUAllocator.cpp:305] Memory block of unknown size was allocated before the profiling started, profiler results will not include the deallocation event\n",
      "[epoch 0, iterations   100] loss: 2.291 accuracy: 8.250000 %[epoch 0, iterations   100] loss: 2.296 accuracy: 11.250000 %[epoch 0, iterations   100] loss: 2.304 accuracy: 10.750000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   100] loss: 2.299 accuracy: 7.750000 %\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 138\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 138\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 138\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 138\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2025-03-28 05:17:35,971] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 32768 to 16384.0\n",
      "[2025-03-28 05:17:35,971] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 32768, reducing to 16384.0\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 173\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 173\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 173\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 173\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2025-03-28 05:17:36,410] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 16384.0 to 8192.0\n",
      "[2025-03-28 05:17:36,410] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 16384.0, reducing to 8192.0\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 184\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 184\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 184\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 184\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:36,543] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:36,543] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations   200] loss: 2.221 accuracy: 12.625000 %[epoch 0, iterations   200] loss: 2.200 accuracy: 10.750000 %[epoch 0, iterations   200] loss: 2.201 accuracy: 12.375000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   200] loss: 2.240 accuracy: 11.125000 %\n",
      "[epoch 0, iterations   300] loss: 2.127 accuracy: 14.500000 %[epoch 0, iterations   300] loss: 2.112 accuracy: 13.916667 %\n",
      "\n",
      "[epoch 0, iterations   300] loss: 2.094 accuracy: 13.583333 %\n",
      "[epoch 0, iterations   300] loss: 2.164 accuracy: 12.000000 %\n",
      "[epoch 0, iterations   400] loss: 2.093 accuracy: 14.937500 %[epoch 0, iterations   400] loss: 2.041 accuracy: 14.500000 %[epoch 0, iterations   400] loss: 2.035 accuracy: 14.937500 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   400] loss: 2.136 accuracy: 13.750000 %\n",
      "[epoch 0, iterations   500] loss: 1.983 accuracy: 16.300000 %\n",
      "[epoch 0, iterations   500] loss: 2.057 accuracy: 16.000000 %[epoch 0, iterations   500] loss: 2.047 accuracy: 15.200000 %\n",
      "\n",
      "[epoch 0, iterations   500] loss: 2.047 accuracy: 14.750000 %\n",
      "[epoch 0, iterations   600] loss: 2.036 accuracy: 17.041667 %[epoch 0, iterations   600] loss: 2.040 accuracy: 15.375000 %[epoch 0, iterations   600] loss: 1.976 accuracy: 17.500000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   600] loss: 2.003 accuracy: 15.583333 %\n",
      "[2025-03-28 05:17:42,955] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:42,955] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:42,955] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:42,956] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:42,956] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:42,956] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:42,956] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:42,956] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[epoch 0, iterations   700] loss: 2.007 accuracy: 18.214286 %[epoch 0, iterations   700] loss: 2.028 accuracy: 16.607143 %\n",
      "\n",
      "[epoch 0, iterations   700] loss: 2.040 accuracy: 17.750000 %\n",
      "[epoch 0, iterations   700] loss: 2.051 accuracy: 16.321429 %\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 702\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 702\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 702\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 702\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:43,170] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:43,170] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations   800] loss: 1.923 accuracy: 17.625000 %[epoch 0, iterations   800] loss: 1.979 accuracy: 18.500000 %[epoch 0, iterations   800] loss: 2.016 accuracy: 18.125000 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   800] loss: 1.967 accuracy: 16.875000 %\n",
      "[epoch 0, iterations   900] loss: 2.024 accuracy: 18.555556 %[epoch 0, iterations   900] loss: 1.961 accuracy: 18.305556 %[epoch 0, iterations   900] loss: 1.987 accuracy: 18.833333 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations   900] loss: 1.942 accuracy: 17.611111 %\n",
      "[epoch 0, iterations  1000] loss: 2.021 accuracy: 18.650000 %[epoch 0, iterations  1000] loss: 1.915 accuracy: 19.225000 %\n",
      "\n",
      "[epoch 0, iterations  1000] loss: 1.923 accuracy: 19.525000 %\n",
      "[epoch 0, iterations  1000] loss: 1.954 accuracy: 18.525000 %\n",
      "[epoch 0, iterations  1100] loss: 1.971 accuracy: 19.772727 %[epoch 0, iterations  1100] loss: 1.942 accuracy: 19.204545 %[epoch 0, iterations  1100] loss: 1.985 accuracy: 19.954545 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1100] loss: 1.903 accuracy: 19.545455 %\n",
      "[epoch 0, iterations  1200] loss: 1.825 accuracy: 19.729167 %[epoch 0, iterations  1200] loss: 1.957 accuracy: 20.229167 %\n",
      "\n",
      "[epoch 0, iterations  1200] loss: 1.954 accuracy: 20.541667 %\n",
      "[epoch 0, iterations  1200] loss: 1.958 accuracy: 20.250000 %\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:49,459] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 4096.0 to 8192.0\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1259\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1259\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1259\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1259\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:50,140] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:50,141] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 8192.0 to 4096.0\n",
      "[2025-03-28 05:17:50,141] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 8192.0, reducing to 4096.0\n",
      "[epoch 0, iterations  1300] loss: 1.799 accuracy: 20.307692 %[epoch 0, iterations  1300] loss: 1.985 accuracy: 20.403846 %[epoch 0, iterations  1300] loss: 1.977 accuracy: 20.846154 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1300] loss: 1.908 accuracy: 20.865385 %\n",
      "[epoch 0, iterations  1400] loss: 1.924 accuracy: 20.660714 %[epoch 0, iterations  1400] loss: 1.896 accuracy: 20.875000 %[epoch 0, iterations  1400] loss: 1.957 accuracy: 21.053571 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1400] loss: 1.905 accuracy: 21.267857 %\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1484\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1484\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1484\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 1484\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:52,886] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:52,886] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  1500] loss: 1.842 accuracy: 21.116667 %[epoch 0, iterations  1500] loss: 1.920 accuracy: 21.183333 %[epoch 0, iterations  1500] loss: 1.898 accuracy: 21.416667 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1500] loss: 1.917 accuracy: 21.766667 %\n",
      "[epoch 0, iterations  1600] loss: 1.844 accuracy: 21.718750 %\n",
      "[epoch 0, iterations  1600] loss: 1.872 accuracy: 21.906250 %\n",
      "[epoch 0, iterations  1600] loss: 1.852 accuracy: 21.812500 %[epoch 0, iterations  1600] loss: 1.906 accuracy: 21.359375 %\n",
      "\n",
      "[epoch 0, iterations  1700] loss: 1.770 accuracy: 22.000000 %[epoch 0, iterations  1700] loss: 1.895 accuracy: 22.044118 %[epoch 0, iterations  1700] loss: 1.858 accuracy: 22.161765 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  1700] loss: 1.933 accuracy: 22.073529 %\n",
      "[epoch 0, iterations  1800] loss: 1.818 accuracy: 22.444444 %[epoch 0, iterations  1800] loss: 1.865 accuracy: 22.611111 %\n",
      "\n",
      "[epoch 0, iterations  1800] loss: 1.859 accuracy: 22.500000 %\n",
      "[epoch 0, iterations  1800] loss: 1.914 accuracy: 22.444444 %\n",
      "[epoch 0, iterations  1900] loss: 1.784 accuracy: 23.013158 %[epoch 0, iterations  1900] loss: 1.825 accuracy: 23.078947 %\n",
      "\n",
      "[epoch 0, iterations  1900] loss: 1.859 accuracy: 22.947368 %\n",
      "[epoch 0, iterations  1900] loss: 1.874 accuracy: 22.657895 %\n",
      "[2025-03-28 05:17:59,025] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:59,025] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:59,025] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:59,025] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:17:59,026] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:17:59,026] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:17:59,026] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:17:59,026] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 0, iterations  2000] loss: 1.739 accuracy: 23.437500 %[epoch 0, iterations  2000] loss: 1.796 accuracy: 23.562500 %\n",
      "\n",
      "[epoch 0, iterations  2000] loss: 1.879 accuracy: 23.062500 %[2025-03-28 05:17:59,193] [INFO] [logging.py:69:log_dist] [Rank 0] step=2000, skipped=6, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "\n",
      "[2025-03-28 05:17:59,194] [INFO] [timer.py:193:stop] 0/2000, SamplesPerSec=1313.372754237179, MemAllocated=0.0GB, MaxMemAllocated=0.0GB\n",
      "[epoch 0, iterations  2000] loss: 1.889 accuracy: 22.937500 %\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2015\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2015\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2015\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2015\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:59,384] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:17:59,384] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  2100] loss: 1.777 accuracy: 23.869048 %[epoch 0, iterations  2100] loss: 1.788 accuracy: 23.797619 %[epoch 0, iterations  2100] loss: 1.806 accuracy: 23.416667 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2100] loss: 1.870 accuracy: 23.214286 %\n",
      "[epoch 0, iterations  2200] loss: 1.854 accuracy: 24.284091 %[epoch 0, iterations  2200] loss: 1.728 accuracy: 24.295455 %[epoch 0, iterations  2200] loss: 1.736 accuracy: 23.897727 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2200] loss: 1.787 accuracy: 23.488636 %\n",
      "[epoch 0, iterations  2300] loss: 1.780 accuracy: 24.586957 %[epoch 0, iterations  2300] loss: 1.745 accuracy: 24.630435 %\n",
      "\n",
      "[epoch 0, iterations  2300] loss: 1.793 accuracy: 24.195652 %\n",
      "[epoch 0, iterations  2300] loss: 1.790 accuracy: 23.826087 %\n",
      "[epoch 0, iterations  2400] loss: 1.843 accuracy: 24.854167 %[epoch 0, iterations  2400] loss: 1.725 accuracy: 24.968750 %[epoch 0, iterations  2400] loss: 1.754 accuracy: 24.552083 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2400] loss: 1.765 accuracy: 24.281250 %\n",
      "[epoch 0, iterations  2500] loss: 1.741 accuracy: 25.170000 %[epoch 0, iterations  2500] loss: 1.800 accuracy: 25.290000 %\n",
      "\n",
      "[epoch 0, iterations  2500] loss: 1.750 accuracy: 25.090000 %\n",
      "[epoch 0, iterations  2500] loss: 1.750 accuracy: 24.680000 %\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:05,420] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2545\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2545\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2545\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 2545\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:05,760] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:05,761] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 0, iterations  2600] loss: 1.733 accuracy: 25.500000 %[epoch 0, iterations  2600] loss: 1.766 accuracy: 25.663462 %[epoch 0, iterations  2600] loss: 1.730 accuracy: 25.413462 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2600] loss: 1.716 accuracy: 24.913462 %\n",
      "[epoch 0, iterations  2700] loss: 1.719 accuracy: 25.944444 %[epoch 0, iterations  2700] loss: 1.765 accuracy: 25.944444 %[epoch 0, iterations  2700] loss: 1.771 accuracy: 25.601852 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2700] loss: 1.841 accuracy: 25.129630 %\n",
      "[epoch 0, iterations  2800] loss: 1.800 accuracy: 26.178571 %[epoch 0, iterations  2800] loss: 1.745 accuracy: 26.241071 %[epoch 0, iterations  2800] loss: 1.726 accuracy: 25.946429 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  2800] loss: 1.822 accuracy: 25.339286 %\n",
      "[epoch 0, iterations  2900] loss: 1.667 accuracy: 26.577586 %[epoch 0, iterations  2900] loss: 1.783 accuracy: 26.310345 %\n",
      "[epoch 0, iterations  2900] loss: 1.690 accuracy: 26.284483 %\n",
      "\n",
      "[epoch 0, iterations  2900] loss: 1.869 accuracy: 25.603448 %\n",
      "[epoch 0, iterations  3000] loss: 1.754 accuracy: 26.558333 %[epoch 0, iterations  3000] loss: 1.685 accuracy: 26.941667 %[epoch 0, iterations  3000] loss: 1.705 accuracy: 26.608333 %\n",
      "\n",
      "\n",
      "[epoch 0, iterations  3000] loss: 1.822 accuracy: 25.808333 %\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:11,987] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 0, iterations  3100] loss: 1.751 accuracy: 26.766129 %[epoch 0, iterations  3100] loss: 1.748 accuracy: 26.943548 %\n",
      "[epoch 0, iterations  3100] loss: 1.734 accuracy: 27.225806 %\n",
      "\n",
      "[epoch 0, iterations  3100] loss: 1.730 accuracy: 26.169355 %\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3168\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3168\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3168\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3168\n",
      "[2025-03-28 05:18:13,797] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:13,797] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations   100] loss: 1.628 accuracy: 35.250000 %[epoch 1, iterations   100] loss: 1.636 accuracy: 37.000000 %[epoch 1, iterations   100] loss: 1.726 accuracy: 35.250000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   100] loss: 1.650 accuracy: 37.250000 %\n",
      "[epoch 1, iterations   200] loss: 1.622 accuracy: 37.250000 %[epoch 1, iterations   200] loss: 1.798 accuracy: 34.750000 %[epoch 1, iterations   200] loss: 1.756 accuracy: 35.375000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   200] loss: 1.646 accuracy: 38.625000 %\n",
      "[epoch 1, iterations   300] loss: 1.698 accuracy: 37.583333 %[epoch 1, iterations   300] loss: 1.731 accuracy: 34.250000 %[epoch 1, iterations   300] loss: 1.730 accuracy: 35.333333 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   300] loss: 1.629 accuracy: 38.250000 %\n",
      "[epoch 1, iterations   400] loss: 1.638 accuracy: 37.500000 %[epoch 1, iterations   400] loss: 1.601 accuracy: 37.312500 %[epoch 1, iterations   400] loss: 1.674 accuracy: 35.250000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   400] loss: 1.636 accuracy: 38.125000 %\n",
      "[epoch 1, iterations   500] loss: 1.636 accuracy: 37.400000 %[epoch 1, iterations   500] loss: 1.742 accuracy: 36.950000 %\n",
      "\n",
      "[epoch 1, iterations   500] loss: 1.597 accuracy: 38.400000 %\n",
      "[epoch 1, iterations   500] loss: 1.538 accuracy: 36.200000 %\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:20,107] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3703\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3703\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3703\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 3703\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:20,533] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:20,533] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations   600] loss: 1.600 accuracy: 37.125000 %[epoch 1, iterations   600] loss: 1.662 accuracy: 37.500000 %[epoch 1, iterations   600] loss: 1.702 accuracy: 36.833333 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   600] loss: 1.624 accuracy: 38.291667 %\n",
      "[epoch 1, iterations   700] loss: 1.626 accuracy: 37.821429 %[epoch 1, iterations   700] loss: 1.681 accuracy: 37.607143 %[epoch 1, iterations   700] loss: 1.640 accuracy: 37.000000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   700] loss: 1.685 accuracy: 37.857143 %\n",
      "[epoch 1, iterations   800] loss: 1.669 accuracy: 37.718750 %[epoch 1, iterations   800] loss: 1.583 accuracy: 38.562500 %[epoch 1, iterations   800] loss: 1.661 accuracy: 37.250000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   800] loss: 1.611 accuracy: 38.281250 %\n",
      "[2025-03-28 05:18:24,228] [INFO] [logging.py:69:log_dist] [Rank 0] step=4000, skipped=10, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2025-03-28 05:18:24,229] [INFO] [timer.py:193:stop] 0/4000, SamplesPerSec=1319.5527526279463, MemAllocated=0.0GB, MaxMemAllocated=0.0GB\n",
      "[epoch 1, iterations   900] loss: 1.649 accuracy: 37.861111 %[epoch 1, iterations   900] loss: 1.649 accuracy: 38.388889 %[epoch 1, iterations   900] loss: 1.633 accuracy: 37.388889 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations   900] loss: 1.718 accuracy: 37.944444 %\n",
      "[epoch 1, iterations  1000] loss: 1.586 accuracy: 38.000000 %[epoch 1, iterations  1000] loss: 1.653 accuracy: 37.675000 %\n",
      "\n",
      "[epoch 1, iterations  1000] loss: 1.663 accuracy: 38.650000 %\n",
      "[epoch 1, iterations  1000] loss: 1.627 accuracy: 38.775000 %\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:26,666] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[epoch 1, iterations  1100] loss: 1.620 accuracy: 38.159091 %[epoch 1, iterations  1100] loss: 1.584 accuracy: 38.613636 %[epoch 1, iterations  1100] loss: 1.622 accuracy: 37.681818 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1100] loss: 1.605 accuracy: 39.022727 %\n",
      "[2025-03-28 05:18:27,498] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4274\n",
      "[2025-03-28 05:18:27,498] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4274\n",
      "[2025-03-28 05:18:27,498] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4274\n",
      "[2025-03-28 05:18:27,498] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4274\n",
      "[2025-03-28 05:18:27,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:27,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:27,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:27,499] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:27,499] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  1200] loss: 1.721 accuracy: 38.041667 %[epoch 1, iterations  1200] loss: 1.551 accuracy: 38.916667 %\n",
      "\n",
      "[epoch 1, iterations  1200] loss: 1.641 accuracy: 37.916667 %\n",
      "[epoch 1, iterations  1200] loss: 1.553 accuracy: 39.208333 %\n",
      "[epoch 1, iterations  1300] loss: 1.749 accuracy: 37.903846 %[epoch 1, iterations  1300] loss: 1.518 accuracy: 39.423077 %[epoch 1, iterations  1300] loss: 1.746 accuracy: 37.711538 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1300] loss: 1.603 accuracy: 39.192308 %\n",
      "[epoch 1, iterations  1400] loss: 1.636 accuracy: 37.857143 %[epoch 1, iterations  1400] loss: 1.660 accuracy: 39.500000 %[epoch 1, iterations  1400] loss: 1.679 accuracy: 37.678571 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1400] loss: 1.579 accuracy: 39.071429 %\n",
      "[epoch 1, iterations  1500] loss: 1.600 accuracy: 37.833333 %[epoch 1, iterations  1500] loss: 1.633 accuracy: 39.616667 %[epoch 1, iterations  1500] loss: 1.669 accuracy: 37.866667 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1500] loss: 1.643 accuracy: 39.200000 %\n",
      "[epoch 1, iterations  1600] loss: 1.697 accuracy: 37.953125 %[epoch 1, iterations  1600] loss: 1.532 accuracy: 37.875000 %[epoch 1, iterations  1600] loss: 1.644 accuracy: 39.500000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1600] loss: 1.583 accuracy: 39.453125 %\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:33,562] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4791\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4791\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4791\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 4791\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:33,756] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:33,756] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  1700] loss: 1.520 accuracy: 38.411765 %[epoch 1, iterations  1700] loss: 1.559 accuracy: 39.823529 %[epoch 1, iterations  1700] loss: 1.626 accuracy: 38.073529 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1700] loss: 1.640 accuracy: 39.602941 %\n",
      "[epoch 1, iterations  1800] loss: 1.595 accuracy: 38.138889 %[epoch 1, iterations  1800] loss: 1.568 accuracy: 39.777778 %[epoch 1, iterations  1800] loss: 1.536 accuracy: 38.541667 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  1800] loss: 1.615 accuracy: 39.694444 %\n",
      "[epoch 1, iterations  1900] loss: 1.579 accuracy: 39.973684 %[epoch 1, iterations  1900] loss: 1.687 accuracy: 38.184211 %\n",
      "\n",
      "[epoch 1, iterations  1900] loss: 1.588 accuracy: 38.750000 %\n",
      "[epoch 1, iterations  1900] loss: 1.584 accuracy: 39.684211 %\n",
      "[epoch 1, iterations  2000] loss: 1.560 accuracy: 40.212500 %[epoch 1, iterations  2000] loss: 1.599 accuracy: 38.462500 %[epoch 1, iterations  2000] loss: 1.585 accuracy: 38.812500 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2000] loss: 1.647 accuracy: 39.737500 %\n",
      "[epoch 1, iterations  2100] loss: 1.561 accuracy: 38.607143 %[epoch 1, iterations  2100] loss: 1.580 accuracy: 39.071429 %[epoch 1, iterations  2100] loss: 1.546 accuracy: 40.285714 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2100] loss: 1.702 accuracy: 39.595238 %\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:39,943] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5295\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5295\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5295\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5295\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:39,982] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:39,982] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  2200] loss: 1.584 accuracy: 38.693182 %[epoch 1, iterations  2200] loss: 1.631 accuracy: 39.215909 %[epoch 1, iterations  2200] loss: 1.574 accuracy: 40.306818 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2200] loss: 1.548 accuracy: 39.818182 %\n",
      "[epoch 1, iterations  2300] loss: 1.575 accuracy: 38.663043 %[epoch 1, iterations  2300] loss: 1.567 accuracy: 39.434783 %[epoch 1, iterations  2300] loss: 1.564 accuracy: 40.413043 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2300] loss: 1.470 accuracy: 40.086957 %\n",
      "[epoch 1, iterations  2400] loss: 1.637 accuracy: 39.520833 %[epoch 1, iterations  2400] loss: 1.618 accuracy: 38.729167 %[epoch 1, iterations  2400] loss: 1.512 accuracy: 40.666667 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2400] loss: 1.536 accuracy: 40.229167 %\n",
      "[epoch 1, iterations  2500] loss: 1.521 accuracy: 39.180000 %[epoch 1, iterations  2500] loss: 1.558 accuracy: 39.640000 %[epoch 1, iterations  2500] loss: 1.605 accuracy: 40.800000 %\n",
      "\n",
      "\n",
      "[epoch 1, iterations  2500] loss: 1.548 accuracy: 40.320000 %\n",
      "[epoch 1, iterations  2600] loss: 1.561 accuracy: 39.269231 %\n",
      "[epoch 1, iterations  2600] loss: 1.501 accuracy: 39.826923 %[epoch 1, iterations  2600] loss: 1.626 accuracy: 40.807692 %\n",
      "\n",
      "[epoch 1, iterations  2600] loss: 1.579 accuracy: 40.278846 %\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:392:_update_scale] No Grad overflow for 500 iterations\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:46,354] [INFO] [fused_optimizer.py:394:_update_scale] Increasing dynamic loss scale from 2048.0 to 4096.0\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5802\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5802\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5802\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:382:_update_scale] \n",
      "Grad overflow on iteration 5802\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:46,429] [INFO] [fused_optimizer.py:383:_update_scale] Reducing dynamic loss scale from 4096.0 to 2048.0\n",
      "[2025-03-28 05:18:46,429] [INFO] [logging.py:69:log_dist] [Rank 0] Overflow detected. Skipping step. Attempted loss scale: 4096.0, reducing to 2048.0\n",
      "[epoch 1, iterations  2700] loss: 1.589 accuracy: 39.277778 %\n",
      "[epoch 1, iterations  2700] loss: 1.558 accuracy: 39.962963 %[epoch 1, iterations  2700] loss: 1.609 accuracy: 40.787037 %\n",
      "\n",
      "[epoch 1, iterations  2700] loss: 1.585 accuracy: 40.361111 %\n",
      "[epoch 1, iterations  2800] loss: 1.536 accuracy: 39.437500 %[epoch 1, iterations  2800] loss: 1.542 accuracy: 40.812500 %\n",
      "\n",
      "[epoch 1, iterations  2800] loss: 1.662 accuracy: 39.964286 %\n",
      "[epoch 1, iterations  2800] loss: 1.669 accuracy: 40.339286 %\n",
      "[2025-03-28 05:18:48,868] [INFO] [logging.py:69:log_dist] [Rank 0] step=6000, skipped=14, lr=[0.001, 0.001], mom=[[0.8, 0.999], [0.8, 0.999]]\n",
      "[2025-03-28 05:18:48,868] [INFO] [timer.py:193:stop] 0/6000, SamplesPerSec=1325.4828434753008, MemAllocated=0.0GB, MaxMemAllocated=0.0GB\n",
      "[epoch 1, iterations  2900] loss: 1.538 accuracy: 39.637931 %\n",
      "[epoch 1, iterations  2900] loss: 1.555 accuracy: 40.008621 %[epoch 1, iterations  2900] loss: 1.544 accuracy: 40.853448 %\n",
      "\n",
      "[epoch 1, iterations  2900] loss: 1.690 accuracy: 40.293103 %\n",
      "[epoch 1, iterations  3000] loss: 1.552 accuracy: 39.691667 %[epoch 1, iterations  3000] loss: 1.562 accuracy: 40.083333 %\n",
      "\n",
      "[epoch 1, iterations  3000] loss: 1.554 accuracy: 40.941667 %\n",
      "[epoch 1, iterations  3000] loss: 1.661 accuracy: 40.275000 %\n",
      "[epoch 1, iterations  3100] loss: 1.577 accuracy: 39.774194 %[epoch 1, iterations  3100] loss: 1.539 accuracy: 40.250000 %\n",
      "\n",
      "[epoch 1, iterations  3100] loss: 1.586 accuracy: 40.927419 %\n",
      "[epoch 1, iterations  3100] loss: 1.508 accuracy: 40.427419 %\n",
      "Training Done\n",
      "Training Done\n",
      "Training Done\n",
      "Training Done\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:188: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(capacity_factor),\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:189: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  torch.tensor(min_capacity))\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:196: TracerWarning: Converting a tensor to a Python integer might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  num_experts = int(gates.shape[1])\n",
      "/opt/conda/lib/python3.8/site-packages/deepspeed/moe/sharded_moe.py:232: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  assert logits.shape[0] >= min_capacity, \"No. of tokens (batch-size) should be greater than min_capacity. Either set min_capacity to 0 or increase your batch size.\"\n",
      "/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:983: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 2.720672607421875 at index (3, 9) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1297.5813586097947 at index (2, 7) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:983: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 4.0255126953125 at index (3, 1) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 1680.1658767772512 at index (0, 7) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:983: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 5.299797058105469 at index (3, 6) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 493.7135749822317 at index (3, 6) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "/opt/conda/lib/python3.8/site-packages/torch/jit/_trace.py:983: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the repeated trace. Detailed error:\n",
      "Tensor-likes are not close!\n",
      "\n",
      "Mismatched elements: 20 / 40 (50.0%)\n",
      "Greatest absolute difference: 6.371002197265625 at index (1, 8) (up to 1e-05 allowed)\n",
      "Greatest relative difference: 10125.761453396524 at index (1, 7) (up to 1e-05 allowed)\n",
      "  _check_trace(\n",
      "Accuracy of the network on the 10000 test images: 42.420000 %\n",
      "Accuracy of plane : 46.500000 %\n",
      "Accuracy of   car : 55.600000 %\n",
      "Accuracy of  bird : 28.000000 %\n",
      "Accuracy of   cat : 14.600000 %\n",
      "Accuracy of  deer : 47.500000 %\n",
      "Accuracy of   dog : 36.000000 %\n",
      "Accuracy of  frog : 44.200000 %\n",
      "Accuracy of horse : 53.400000 %\n",
      "Accuracy of  ship : 54.300000 %\n",
      "Accuracy of truck : 44.100000 %\n",
      "Evaluation Done\n",
      "Accuracy of the network on the 10000 test images: 43.340000 %\n",
      "Accuracy of plane : 46.000000 %\n",
      "Accuracy of   car : 57.000000 %\n",
      "Accuracy of  bird : 27.900000 %\n",
      "Accuracy of   cat : 16.400000 %\n",
      "Accuracy of  deer : 47.600000 %\n",
      "Accuracy of   dog : 38.000000 %\n",
      "Accuracy of  frog : 43.100000 %\n",
      "Accuracy of horse : 54.400000 %\n",
      "Accuracy of  ship : 55.900000 %\n",
      "Accuracy of truck : 47.100000 %\n",
      "Evaluation Done\n",
      "Accuracy of the network on the 10000 test images: 42.980000 %\n",
      "Accuracy of plane : 45.700000 %\n",
      "Accuracy of   car : 57.400000 %\n",
      "Accuracy of  bird : 28.000000 %\n",
      "Accuracy of   cat : 14.800000 %\n",
      "Accuracy of  deer : 48.400000 %\n",
      "Accuracy of   dog : 37.000000 %\n",
      "Accuracy of  frog : 45.700000 %\n",
      "Accuracy of horse : 54.800000 %\n",
      "Accuracy of  ship : 53.700000 %\n",
      "Accuracy of truck : 44.300000 %\n",
      "Evaluation Done\n",
      "[2025-03-28 05:19:06,442] [INFO] [launch.py:210:main] Process 16048 exits successfully.\n",
      "[2025-03-28 05:19:06,442] [INFO] [launch.py:210:main] Process 16047 exits successfully.\n",
      "[2025-03-28 05:19:07,443] [INFO] [launch.py:210:main] Process 16046 exits successfully.\n"
     ]
    }
   ],
   "source": [
    "! deepspeed --num_gpus=4 /dli/code/moe/cifar10_deepspeed_MOE.py  \\\n",
    "    --deepspeed \\\n",
    "    --deepspeed_config /dli/code/moe/ds_config.json \\\n",
    "    --moe \\\n",
    "    --ep-world-size 4 \\\n",
    "    --num-experts-per-layer 8 \\\n",
    "    --top-k 1 \\\n",
    "    --noisy-gate-policy 'RSample' \\\n",
    "    --moe-param-group \\\n",
    "    --profile-execution=True \\\n",
    "    --profile-name='zero0_MOE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UoqB6aQkfpb6"
   },
   "source": [
    "<img src=\"images/deepspeed_MOE.png\" width=\"950\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LFyU2_o_fpb9"
   },
   "source": [
    "---\n",
    "<h2 style=\"color:green;\">축하합니다!</h2>\n",
    "\n",
    "다음 실습은 대형 신경망을 배포하는 데 초점을 맞출 것입니다.\n",
    "\n",
    "다음으로 넘어가기 전에 대기열에서 실행 중이거나 대기 중인 작업이 없는지 확인해야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 26,
     "status": "ok",
     "timestamp": 1661300640953,
     "user": {
      "displayName": "Solee Moon KR",
      "userId": "15448835692925759007"
     },
     "user_tz": -540
    },
    "id": "eH9FenF4fpb_",
    "outputId": "aa978334-a6e7-4713-e24e-da18f65afb7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Check the SLURM jobs queue \n",
    "!squeue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rie0B5N6fpcA"
   },
   "source": [
    "아직 실행 중이거나 보류 중인 작업이 있는 경우 다음 셀을 실행하고 `scancel` 명령을 사용하여 모든 어드민 사용자의 작업을 취소합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 21,
     "status": "ok",
     "timestamp": 1661300640954,
     "user": {
      "displayName": "Solee Moon KR",
      "userId": "15448835692925759007"
     },
     "user_tz": -540
    },
    "id": "CQpgPanDfpcB",
    "outputId": "edfd2285-b559-47e8-f5ea-8f85772dd4b4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n"
     ]
    }
   ],
   "source": [
    "# Cancel admin user jobs\n",
    "! scancel -u $USER\n",
    "\n",
    "# Check again the SLURM jobs queue (should be either empty, or the status TS column should be CG)\n",
    "! squeue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "06_MOE_alternative_models.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
